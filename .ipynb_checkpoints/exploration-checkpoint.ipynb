{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Luke Waninger\n",
    "Exploring a dataset containing call center tickets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-18T23:07:52.764492Z",
     "start_time": "2018-07-18T23:07:52.725858Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-4fa07210c7db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'IPCompleter.greedy=True'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnotebook_init\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/lukew/OneDrive/git/CallCenterExploration/notebook_init.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmultiprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mProcess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQueue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_objs\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'plotly'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config IPCompleter.greedy=True\n",
    "\n",
    "from notebook_init import *\n",
    "import datetime as dt\n",
    "import itertools\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import multiprocessing\n",
    "from multiprocessing import Lock, Manager, Process, Queue\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objs as go\n",
    "from plotly import tools\n",
    "import plotly.figure_factory as figf\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "from scipy.stats import mode, norm\n",
    "from sklearn.linear_model import Lasso, LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "from tqdm import tqdm, tqdm_pandas\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "proc_manager = Manager()\n",
    "tqdm.pandas()\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "UNKNOWN = 'unk'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Notes:\n",
    "* change the `cpu_count` variable below if you need to use your computer for other tasks while this notebook runs\n",
    "* to view visualizations this notebook must be ran in Trusted mode\n",
    "* Python version = 3.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-18T23:06:47.937626Z",
     "start_time": "2018-07-18T23:06:43.962Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cpu_count = multiprocessing.cpu_count()\n",
    "print(f'cores to utilize:   {cpu_count} of {multiprocessing.cpu_count()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Load and clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-18T23:06:47.939094Z",
     "start_time": "2018-07-18T23:06:43.967Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('tickets.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, we see that most of the raw features are categorical or ordinal variables. In fact, the only quantitative variable is $\\texttt{Tenure_Months}$. Another interesting characteristic of this data is the logical division of sources. On the left we have information regarding the ticket while the right contains information regarding either the agent on call or the individual who generated the ticket. Intrinsically, we have a one to many relationship between tickets and personnel. Separating these two datasets with correct associations gives more room for feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-18T23:06:47.942110Z",
     "start_time": "2018-07-18T23:06:43.975Z"
    },
    "code_folding": [
     1,
     12,
     21,
     31,
     37,
     50
    ]
   },
   "outputs": [],
   "source": [
    "df.columns = list(map(str.lower, df.columns))\n",
    "col_map = {\n",
    "    'issue_resolution': 'resolution',\n",
    "    'issue_category':   'category',\n",
    "    'issue_severity':   'severity',\n",
    "    'support_level':    'level',\n",
    "    'support_channel':  'channel',\n",
    "    'tenure_months':    'tenure',\n",
    "    'career_desc':      'career'\n",
    "}\n",
    "df.rename(columns=col_map, inplace=True)\n",
    "\n",
    "tick_cols = [\n",
    "    'unnamed: 0', \n",
    "    'resolution', \n",
    "    'category', \n",
    "    'severity', \n",
    "    'level',\n",
    "    'channel',  \n",
    "]\n",
    "\n",
    "tech_cols = [\n",
    "    'building', \n",
    "    'city', \n",
    "    'position', \n",
    "    'country', \n",
    "    'tenure', \n",
    "    'career', \n",
    "    'division'\n",
    "]\n",
    "\n",
    "numeric_cols = [\n",
    "    'unnamed: 0',\n",
    "    'tenure'\n",
    "]\n",
    "\n",
    "# strip whitespace and convert all cells to lowercase\n",
    "for col in df.columns:\n",
    "    if col in numeric_cols:\n",
    "        continue\n",
    "    \n",
    "    else:\n",
    "        df[col] = df[col].str.strip()\n",
    "        df[col] = df[col].str.lower()\n",
    "\n",
    "# separate tickets and technicians into separate dataframes\n",
    "tickets = df.loc[:, tick_cols]\n",
    "technicians = df.loc[:, tech_cols]\n",
    "\n",
    "# check whether the unnamed ticket column is a valid key, rename if so\n",
    "if (len(df.loc[:, 'unnamed: 0']) == len(pd.unique(df.loc[:, 'unnamed: 0']))):\n",
    "    tmap = {'unnamed: 0':'ticket_id'}\n",
    "    tickets.rename(columns=tmap, inplace=True)\n",
    "    df.rename(columns=tmap, inplace=True)\n",
    "    tick_cols[0] = 'ticket_id'\n",
    "   \n",
    "# drop duplicate technicians\n",
    "technicians = technicians.drop_duplicates(keep='first')\n",
    "\n",
    "tickets.head(5)\n",
    "technicians.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column `unnamed: 0` turned out to be valid key which further implies that no duplicate tickets existed in the frame. Before we do any further data reorganization we need to create a foreign key from ticket to technician in order to maintain data integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-18T23:06:47.944086Z",
     "start_time": "2018-07-18T23:06:43.981Z"
    }
   },
   "outputs": [],
   "source": [
    "tmp = dict()\n",
    "\n",
    "techs = []\n",
    "for idx, row in tqdm(\n",
    "    zip(df.index.tolist(), df.iterrows()), \n",
    "    total=len(df), \n",
    "    desc='generating keys'\n",
    "):\n",
    "    key = '.'.join([str(a) for a in row[1][tech_cols]])\n",
    "    \n",
    "    if key not in tmp.keys():\n",
    "        tmp[key] = idx\n",
    "        val = idx\n",
    "    else:\n",
    "        val = tmp[key]\n",
    "    \n",
    "    techs.append(val)\n",
    "\n",
    "tickets['technician'] = techs\n",
    "tickets.head(5)\n",
    "print(len(pd.unique(tickets.technician)))\n",
    "\n",
    "del tmp, techs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've found around 2500 duplicate persons and mapped them as foreign keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-18T23:06:47.945530Z",
     "start_time": "2018-07-18T23:06:43.988Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.crosstab(tickets.resolution, tickets.category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Support tickets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-18T23:06:47.948084Z",
     "start_time": "2018-07-18T23:06:43.993Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "evaluation(tickets, exclusions=['ticket_id', 'technician'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We immediately see several commonalities among the responses that we can take advantage of. Particularly, the signal regarding our research question can be increased by binning appropriate columns and reducing the number of features. Additionally, the 'severity' column has an embedded ordering so I'll change it to integer values and treat it as an ordinal variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-18T23:06:47.950300Z",
     "start_time": "2018-07-18T23:06:43.998Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# adjust the misspelling\n",
    "edu_forms = ['user education', 'user edacution', 'user educate']\n",
    "tickets['edu'] = tickets.resolution.isin(edu_forms).astype(int)\n",
    "\n",
    "# drop resolution in favor of a binary 'is_edu'\n",
    "tickets = tickets.drop(labels=['resolution'], axis=1)\n",
    "\n",
    "# group the tiny categories\n",
    "tickets.loc[tickets.category.isin(['accounts', 'software', 'other']) | tickets.category.isnull(), 'category'] = 'other'\n",
    "\n",
    "# adjust the severity to make use of the implicit ordering and reset\n",
    "# the single sev4 to sev3\n",
    "tickets.loc[tickets.severity == 'sev1', 'severity'] = 1\n",
    "tickets.loc[tickets.severity == 'sev2', 'severity'] = 2\n",
    "tickets.loc[tickets.severity == 'sev3', 'severity'] = 3\n",
    "tickets.loc[tickets.severity == 'sev4', 'severity'] = 3\n",
    "\n",
    "# set the index to ticket_ids\n",
    "tickets.index = tickets.ticket_id\n",
    "tickets.drop(labels=['ticket_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-18T23:06:47.952182Z",
     "start_time": "2018-07-18T23:06:44.003Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f'within support level, {np.round(np.sum(tickets.level.isna()/len(tickets)), 3)*100}% are nan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The support_level column is almost exclusively NaN. Without background knowledge these values could come from two sources: one, the caller didn't have a support plan or two, the data was lost somewhere along the way. For this project, I'm assuming the first and setting the NaNs to 'none'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-18T23:06:47.954275Z",
     "start_time": "2018-07-18T23:06:44.007Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tickets.loc[tickets.level.isnull(), 'level'] = 'none'\n",
    "\n",
    "evaluation(tickets, exclusions=['technician'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Technicians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### initial munging and imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-18T23:06:47.956174Z",
     "start_time": "2018-07-18T23:06:44.012Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "evaluation(technicians, exclusions=[\n",
    "    'city', 'country', 'division', 'tenure', 'position', 'building'\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Fortunately, there aren't many missing values and of those, city and country might be linear combinations of other fields. First, impute the missing cities and countries by their inherent hierarchy. Find matching buildings and fall back to countries that must match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-18T23:06:47.957692Z",
     "start_time": "2018-07-18T23:06:44.016Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "missing_cities = technicians.loc[technicians.city.isnull(), :]\n",
    "match_count = 0\n",
    "\n",
    "# iterate through each row looking for matching cities by the building\n",
    "# excluding the values 'home office' and 'mobile'\n",
    "for idx, mc in missing_cities.iterrows():\n",
    "    if mc.building == 'home office' or mc.building == 'mobile':\n",
    "        technicians.loc[idx, 'city'] = UNKNOWN\n",
    "        \n",
    "    matches = technicians.loc[\n",
    "        (~technicians.city.isnull()) &\n",
    "        (technicians.country == mc.country)\n",
    "    ]\n",
    "    \n",
    "    # group by the building, find, and set the city\n",
    "    grouped = matches.groupby(by=['building'], axis=0)\n",
    "    if mc.building in grouped.groups.keys():\n",
    "        \n",
    "        # get the group of technicians that work in the same building\n",
    "        group = technicians.loc[grouped.groups[mc.building], :]\n",
    "        \n",
    "        # find and set the matching city\n",
    "        city = pd.Series(group.city.value_counts()).values.argmax\n",
    "        city = group.city.values[city()]\n",
    "        \n",
    "        technicians.loc[idx, 'city'] = city\n",
    "        match_count += 1\n",
    "    else:\n",
    "        technicians.loc[idx, 'city'] = UNKNOWN\n",
    "\n",
    "print(f'{match_count} match(es) found\\n')\n",
    "evaluation(technicians, exclusions=tech_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Next, impute missing countries by referencing technicians with matching cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-18T23:06:47.960237Z",
     "start_time": "2018-07-18T23:06:44.019Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "missing_countries = technicians.loc[technicians.country.isnull(), :]\n",
    "match_count = 0\n",
    "\n",
    "# iterate through each row looking for matching cities\n",
    "for idx, mc in missing_countries.iterrows():        \n",
    "    matches = technicians.loc[\n",
    "        (technicians.city != UNKNOWN) &\n",
    "        (technicians.city == mc.city)\n",
    "    ]\n",
    "    \n",
    "    # group by the city and find the country it's in\n",
    "    grouped = matches.groupby(by=['city'], axis=0)\n",
    "    if mc.city in grouped.groups.keys():\n",
    "        # get the matching city, then set the matching country\n",
    "        group = technicians.loc[grouped.groups[mc.city], :]\n",
    "        country = pd.Series(group.country.value_counts()).values.argmax\n",
    "        country = group.country.values[country()]\n",
    "        technicians.loc[idx, 'country'] = country\n",
    "        match_count += 1\n",
    "    else:\n",
    "        technicians.loc[idx, 'country'] = UNKNOWN\n",
    "    \n",
    "print(f'{match_count} match(es) found\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-18T23:06:47.962732Z",
     "start_time": "2018-07-18T23:06:44.022Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# create a ticket count\n",
    "ticket_counts = pd.DataFrame(tickets.groupby(['technician']).aggregate(len)).iloc[:, 0]\n",
    "technicians['ticket_count'] = np.ones(len(technicians))\n",
    "\n",
    "for i, v in ticket_counts.iteritems():\n",
    "    technicians.loc[i, 'ticket_count'] = v\n",
    "\n",
    "# create binary variables for executive, managers, and ic\n",
    "exec_ = ['exec6', 'exec5', 'exec4', 'exec3']\n",
    "technicians['is_exec'] = [1 if x in exec_ else 0 for x in technicians.career.values]\n",
    "\n",
    "mgr = ['mgr6', 'mgr5', 'mgr4']\n",
    "technicians['is_mgr'] = [1 if x in mgr else 0 for x in technicians.career.values]\n",
    "\n",
    "ic = ['ic7', 'ic6', 'ic5']\n",
    "technicians['is_ic'] = [1 if x in ic else 0 for x in technicians.career.values]\n",
    "\n",
    "t = [tech_cols.append(key) for key in ['ticket_count', 'is_exec', 'is_mgr', 'is_ic']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### binning\n",
    "I join the underrepresented categories for two reasons: one, to give them more influence during training and two: to prevent errors during shuffles!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-18T23:06:47.966616Z",
     "start_time": "2018-07-18T23:06:44.029Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "technicians.loc[technicians.career.isin(exec_), 'career'] = 'exec'\n",
    "technicians.loc[technicians.career.isin(mgr), 'career'] = 'mgr4'\n",
    "technicians.loc[technicians.career.isin(ic), 'career'] = 'ic5'\n",
    "\n",
    "# group the building numbers to 'office' and rename 'home office' to 'home'\n",
    "technicians.loc[technicians.building.str.startswith('b'), 'building'] = 'office'\n",
    "technicians.loc[technicians.building == 'home office', 'building'] = 'home'\n",
    "\n",
    "del exec_, mgr, ic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "And finally, tenure has only 6 missing values from what should theoretically be an exponential distribution. We can impute these by finding means of correlated observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-18T23:06:47.969097Z",
     "start_time": "2018-07-18T23:06:44.033Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "technicians.loc[technicians.tenure.isnull(), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-18T23:06:47.974369Z",
     "start_time": "2018-07-18T23:06:44.037Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set the missing tenure values to the mean of associated data points\n",
    "mean_tenure = np.mean(technicians.loc[\n",
    "    ((technicians.country == 'ph') |\n",
    "    (technicians.country == 'us')) & \n",
    "    ((technicians.building == 'mobile') |\n",
    "    (technicians.building == 'b0535')), 'tenure']\n",
    ")\n",
    "technicians.loc[technicians.tenure.isnull(), 'tenure'] = mean_tenure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use LASSO to impute the missing careers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use LASSO regression with this report because the features are almost exclusively categorical. Many indicator variables will be made and many of which will have no use in the final model. LASSO gives us a way to completely remove their influence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-18T23:06:47.979211Z",
     "start_time": "2018-07-18T23:06:44.040Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x  = technicians.loc[~technicians.career.isnull(), :].dropna()\n",
    "xp = technicians.loc[technicians.career.isnull(), :].fillna(0)\n",
    "y  = x.career.values\n",
    "\n",
    "# generate a label->int mapping\n",
    "labels = pd.unique(y)\n",
    "label_map   = {i:a for a, i in enumerate(labels)}\n",
    "label_map_r = {a:i for a, i in enumerate(labels)}\n",
    "y = np.array([label_map[yi] for yi in y])\n",
    "\n",
    "# drop the useless features\n",
    "x  =  x.drop(labels=['career'], axis=1)\n",
    "xp = xp.drop(labels=['career'], axis=1)\n",
    "\n",
    "# generate indicator variables\n",
    "x  = pd.get_dummies(x)\n",
    "xp = pd.get_dummies(xp)\n",
    "\n",
    "# add the missing sparse features\n",
    "miss_xp = set(x.columns)-set(xp.columns)\n",
    "miss_x  = set(xp.columns)-set(x.columns)\n",
    "for col in miss_xp:\n",
    "    xp[col] = np.zeros(len(xp))\n",
    "\n",
    "for col in miss_x:\n",
    "    x[col] = np.zeros(len(x))\n",
    "        \n",
    "# split and standardize\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=10)\n",
    "\n",
    "scaler  = StandardScaler().fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test  = scaler.transform(x_test)\n",
    "xp = scaler.transform(xp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T05:47:14.306965Z",
     "start_time": "2018-06-12T05:42:19.733Z"
    }
   },
   "source": [
    "##### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-18T23:06:47.982018Z",
     "start_time": "2018-07-18T23:06:44.042Z"
    },
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "# generate a list of all the OvO pairs we need to train\n",
    "pairs = [p for p in itertools.combinations(np.unique(y), 2)]\n",
    "\n",
    "# setup a progress bar\n",
    "pbar, qu = progress_bar(total=len(pairs), desc='fitting OvO classifiers')\n",
    "                        \n",
    "# setup the classifier\n",
    "parameters = {\n",
    "    'alpha': np.linspace(0.1, 5, 10),\n",
    "    'fit_intercept': [True, False],\n",
    "}\n",
    "\n",
    "# fit using the producer consumer parallel pattern\n",
    "clf = Lasso(max_iter=5000, selection='random', copy_X=True)\n",
    "clfs = prod_con_map(\n",
    "    func=fit_one, \n",
    "    vals=[(pair, qu, clf, parameters) for pair in pairs], \n",
    "    n_cons=cpu_count\n",
    ")\n",
    "\n",
    "# terminate and close the progres bar\n",
    "pbar.terminate()\n",
    "pbar.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T05:47:14.335251Z",
     "start_time": "2018-06-12T05:43:19.895Z"
    }
   },
   "source": [
    "##### validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-18T23:06:47.985534Z",
     "start_time": "2018-07-18T23:06:44.045Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# make predictions\n",
    "def predict_ovo(x):\n",
    "    yhat, rcoefs = [], []\n",
    "    for clf in clfs:\n",
    "        clf, pos, neg = clf\n",
    "        plab, nlab = label_map_r[pos], label_map_r[neg]\n",
    "\n",
    "        # show the remaining number of coefficients so we can get a \n",
    "        # feeling for the effectiveness of our feature space\n",
    "        rc = np.array([i for i, c in enumerate(clf.best_estimator_.coef_) if c > 0])\n",
    "        rcoefs.append(rc)\n",
    "\n",
    "        t = clf.predict(x)\n",
    "        yhat.append([pos if yhi > 0 else neg for yhi in t])\n",
    "\n",
    "    # take the mode of predicted values, breaking ties at random-uniform\n",
    "    yhat = np.array(yhat).T\n",
    "    yhat = [np.random.choice(mode(r).mode, 1)[0] for r in yhat]    \n",
    "\n",
    "    # smoosh the remaining coefficients\n",
    "    coefs = []\n",
    "    for l in rcoefs:\n",
    "        for e in l:\n",
    "            coefs.append(e)\n",
    "    rcoefs = coefs\n",
    "    \n",
    "    return yhat, rcoefs\n",
    "\n",
    "yhat, rcoefs = predict_ovo(x_test)\n",
    "\n",
    "# show the confusion matrix\n",
    "cm = confusion_matrix(y_test, yhat)\n",
    "\n",
    "fig = plot_confusion_matrix(cm, classes=labels)\n",
    "iplot(fig, filename='cm')\n",
    "\n",
    "# print validation score\n",
    "print(f'\\nfinal validation error: {np.mean(yhat != y_test)}')\n",
    "print(f'of {x_train.shape[1]} features, only {len(np.unique(rcoefs))} were found useful for any model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### impute\n",
    "The validation error isn't to bad considering the number of classes we're predicting. Also note that IC4 tends to pull in the IC3 labels which could indicate that we're underfitting for IC4 or overfitting IC3. I tried several down/up sampling techniques to adjust for this but in the decided that a different model might be a better choice. Let's go ahead and impute the remaining null career entries using the trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-18T23:06:47.987704Z",
     "start_time": "2018-07-18T23:06:44.050Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "yhat, tmp = predict_ovo(xp)\n",
    "\n",
    "# set the imputed\n",
    "idx = technicians.career.isnull()\n",
    "technicians.loc[idx, 'career'] = [label_map_r[yhi] for yhi in yhat]\n",
    "\n",
    "del yhat, tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Final data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-18T23:06:47.989557Z",
     "start_time": "2018-07-18T23:06:44.053Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "evaluation(tickets, exclusions=['technician'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-18T23:06:47.991347Z",
     "start_time": "2018-07-18T23:06:44.057Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "evaluation(technicians, exclusions=['city', 'country', 'division', 'tenure', 'position'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I'd like to make a visual assessment of each indicator. Is it clear that any variable behaves differently for education vs. non-education resolution types?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-18T23:06:47.993282Z",
     "start_time": "2018-07-18T23:06:44.063Z"
    },
    "code_folding": [
     2,
     27,
     46
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# plot predictors w/respect to response\n",
    "df = tickets.join(technicians, on='technician', how='outer')\n",
    "predictors = [\n",
    "    'channel', 'severity', 'category', 'level', 'is_exec', 'is_mgr', 'is_ic', 'building'\n",
    "]\n",
    "\n",
    "# setup the main figure\n",
    "fig = tools.make_subplots(\n",
    "    rows=3, cols=5,\n",
    "    specs=[\n",
    "        [{'rowspan':3}, {'colspan':4}, None, None, None],\n",
    "        [None, {}, {}, {}, {}],\n",
    "        [None, {}, {}, {}, {}]\n",
    "    ],\n",
    "    subplot_titles=(\n",
    "            'Resolved through education',\n",
    "            'Proportions of predictors resolved through education'\n",
    "    ),\n",
    "    print_grid=False\n",
    ")\n",
    "\n",
    "# update subplot sizes\n",
    "fig['layout']['yaxis2'].update(domain=[.95, 1])\n",
    "t = [fig['layout'][f'yaxis{i}'].update(domain=[.55, .95]) for i in [3, 4, 5,  6]]\n",
    "t = [fig['layout'][f'yaxis{i}'].update(domain=[0,  .35])  for i in [7, 8, 9, 10]]\n",
    "\n",
    "# remove the extra ink\n",
    "for idx in [4, 5, 6, 8, 9, 10]:\n",
    "    fig['layout'][f'yaxis{idx}'].update(\n",
    "        ticks='',\n",
    "        showline=False,\n",
    "        zeroline=False,\n",
    "        showticklabels=False\n",
    "    )\n",
    "\n",
    "# generate the main leftside bars and add them to the plot\n",
    "mb0 = go.Bar(\n",
    "        name='other resolution types',\n",
    "        x=['no'],\n",
    "        y=tickets.loc[tickets.edu == 0, 'edu'].value_counts(),\n",
    "        marker=dict(\n",
    "            color=Theme().GREY\n",
    "        ),\n",
    "        xaxis='x1',\n",
    "        yaxis='y1'\n",
    "    )\n",
    "mb1 = go.Bar(\n",
    "        name='solved through user education',\n",
    "        x=['yes'],\n",
    "        y=tickets.loc[tickets.edu == 1, 'edu'].value_counts(),\n",
    "        marker=dict(\n",
    "            color=Theme().ORANGE\n",
    "        ),\n",
    "        xaxis='x1',\n",
    "        yaxis='y1'\n",
    "    )\n",
    "\n",
    "t = fig.append_trace(mb0, 1, 1)\n",
    "t = fig.append_trace(mb1, 1, 1)\n",
    "\n",
    "# create the indicator bars\n",
    "col_idx = 2\n",
    "for ax, predictor in enumerate(predictors, 3):\n",
    "    row = 2 if 3 <= ax <= 6 else 3\n",
    "    col = ax-1 if 3 <= ax <= 6 else ax-5\n",
    "    bars = [] \n",
    "    \n",
    "    for edu in [0, 1]:\n",
    "        series = df.loc[df.edu == edu, predictor]\n",
    "        indicators = pd.unique(df[predictor])    \n",
    "\n",
    "        for i, indicator in enumerate(indicators): \n",
    "            a = len(series[series == indicator])\n",
    "            b = len(series)\n",
    "            y = a/(a+b)\n",
    "\n",
    "            b = go.Bar(\n",
    "                name=indicator,\n",
    "                x=['no' if edu == 0 else 'yes'],\n",
    "                y=[y],\n",
    "                text=indicator,\n",
    "                marker=dict(color=Theme()[i]),\n",
    "                xaxis=f'x{ax}',\n",
    "                yaxis=f'y{ax}'\n",
    "            )\n",
    "            \n",
    "            fig.append_trace(b, row, col)\n",
    "    fig['layout'][f'xaxis{ax}'].update(title=predictor)\n",
    "    \n",
    "fig['layout'].update(barmode='stack', showlegend=False)\n",
    "iplot(fig, filename='issue_bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, theres almost no discernible difference between the predictors when separated by response variable. Since we have geolocation data, lets get an idea of where the positive labels are coming from by generating a cross-tabulation/bar plot and choropleth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-18T23:06:47.995397Z",
     "start_time": "2018-07-18T23:06:44.066Z"
    },
    "code_folding": [
     0,
     24,
     33
    ]
   },
   "outputs": [],
   "source": [
    "# mapping country codes to names\n",
    "cc = pd.read_csv('country_codes.csv')\n",
    "cc.columns = ['code', 'name']\n",
    "code_map = {str(code).lower():name for code, name in zip(cc.code.values, cc.name)}\n",
    "\n",
    "df['country'] = [code_map[v] if v in code_map.keys() else v for v in df.country.values]\n",
    "\n",
    "# create the cross-tabulation, normalized\n",
    "ct = pd.crosstab(df.country, df.edu, normalize='index')\n",
    "ct.drop(columns=0, inplace=True)\n",
    "\n",
    "# create a non-normalized cross tab to get the count of tickets per country\n",
    "temp = pd.crosstab(df.country, df.edu)\n",
    "ct['tickets'] = [a+b for a, b in zip(temp.iloc[:, 0], temp.iloc[:, 1])]\n",
    "ct = ct.sort_values(by=1, axis=0, ascending=False)\n",
    "\n",
    "# calculate the entire sample proportion\n",
    "total_prop = np.round(len(df.loc[df['edu'] == 1, :])/len(df.loc[df['edu'] == 0, :]), 3)\n",
    "print(f'The proportion of all tickets closed through education is {total_prop}.')\n",
    "\n",
    "n_countries = 25\n",
    "x = ct.index.values[2:n_countries]\n",
    "y = ct.iloc[2:n_countries, 0]\n",
    "\n",
    "data = go.Bar(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        marker=dict(\n",
    "            color=Theme().GREY\n",
    "        ),\n",
    "        xaxis='x1',\n",
    "        yaxis='y1'\n",
    "    )\n",
    "layout = go.Layout(\n",
    "        title = 'Top performing countries',\n",
    "        xaxis = dict(tickangle = 45),\n",
    "        yaxis = dict(title='Proportion closed through education')\n",
    "    )\n",
    "\n",
    "fig = go.Figure(data=[data], layout=layout)\n",
    "iplot(fig, filename='top_countries')\n",
    "del temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-18T23:06:47.998261Z",
     "start_time": "2018-07-18T23:06:44.069Z"
    },
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# plot by percentage of tickets closed through education\n",
    "data = [\n",
    "    go.Choropleth(\n",
    "        locationmode = 'country names',\n",
    "        locations = ct.index.values,\n",
    "        z = ct.iloc[:, 0],\n",
    "        colorscale = [[0, Theme().TAN],[1, Theme().GREY]],\n",
    "        autocolorscale = False,\n",
    "        marker = dict(\n",
    "            line = dict (\n",
    "                color = 'rgb(180,180,180)',\n",
    "                width = 0.5\n",
    "            )),\n",
    "        geo = 'geo'\n",
    "      )\n",
    "    ]\n",
    "# generate the layout\n",
    "layout = go.Layout(\n",
    "    title = 'Percentage of tickets closed through education',\n",
    "    geo = dict(\n",
    "        scope = 'world',\n",
    "        showcountries = True,\n",
    "        showframe = False,\n",
    "        showcoastlines = True,\n",
    "        showland = True,\n",
    "        landcolor = \"rgb(229, 229, 229)\",\n",
    "        countrycolor = \"rgb(255, 255, 255)\" ,\n",
    "        coastlinecolor = \"rgb(255, 255, 255)\",\n",
    "        projection = dict(\n",
    "            type = 'Mercator'\n",
    "        )\n",
    "    ))\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout )\n",
    "iplot(fig, filename='world_map' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-18T23:06:48.000547Z",
     "start_time": "2018-07-18T23:06:44.072Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# plot by number of tickets\n",
    "data = [\n",
    "    go.Choropleth(\n",
    "        locationmode = 'country names',\n",
    "        locations = ct.index.values,\n",
    "        z = ct.tickets,\n",
    "        colorscale = [[0, Theme().TAN],[1, Theme().GREY]],\n",
    "        autocolorscale = False,\n",
    "        marker = dict(\n",
    "            line = dict (\n",
    "                color = 'rgb(180,180,180)',\n",
    "                width = 0.5\n",
    "            )),\n",
    "        geo = 'geo'\n",
    "      )\n",
    "    ]\n",
    "\n",
    "# generate the layout\n",
    "layout = go.Layout(\n",
    "    title = 'Number of tickets closed',\n",
    "    geo = dict(\n",
    "        scope = 'world',\n",
    "        showcountries = True,\n",
    "        showframe = False,\n",
    "        showcoastlines = True,\n",
    "        showland = True,\n",
    "        landcolor = \"rgb(229, 229, 229)\",\n",
    "        countrycolor = \"rgb(255, 255, 255)\" ,\n",
    "        coastlinecolor = \"rgb(255, 255, 255)\",\n",
    "        projection = dict(\n",
    "            type = 'Mercator'\n",
    "        )\n",
    "    ))\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout )\n",
    "iplot(fig, filename='world_map' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most notable countries are displayed above. The particularly interesting countries are those that also have a considerable amount of tickets. It could be useful for the leadership team to evaluate the difference in business processes between these and countries in the lower proportions.\n",
    "\n",
    "Additionally, the United states has the highest number of tickets and is below the mean. Lets perform a large-value z-test to show how much statistical difference exists.\n",
    "\n",
    "$H_0: \\quad \\hat{p}_{us} - \\hat{p}_{world} = 0$\n",
    "\n",
    "$H_a: \\quad \\hat{p}_{us} - \\hat{p}_{world} \\ne 0$\n",
    "\n",
    "$Z: \\frac{\\hat{p}_{us}-\\hat{p}_{world}}{\\sqrt{\\hat{p}(1-\\hat{p}(1/n_{us}+1/n_{world}))}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-18T23:06:48.003036Z",
     "start_time": "2018-07-18T23:06:44.077Z"
    },
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# run the test\n",
    "us = 'United States of America'\n",
    "pa = ct.loc[us, 1]\n",
    "na = len(df.loc[df.country == us, 'country'])\n",
    "print(f'US proportion: {np.round(pa, 3)}, n: {na}')\n",
    "\n",
    "pb = np.mean(ct.loc[~(ct.index == 'us'), 1])\n",
    "nb = len(df.loc[df.country != us, 'country'])\n",
    "print(f'world proportion: {np.round(pb, 3)}, n: {nb}\\n')\n",
    "\n",
    "n  = na + nb\n",
    "ph = (n*pa+nb*pb)/(na+nb)\n",
    "se = np.sqrt(ph*(1-ph)*(1/na+1/nb))\n",
    "z  = abs(pa-pb)/se\n",
    "p  = 2*norm.sf(abs(z))\n",
    "dif   = pa-pb\n",
    "lower = dif-1.96*se\n",
    "upper = dif+1.96*se\n",
    "\n",
    "print(f'standard error: {np.round(se, 4)}')\n",
    "print(f'z score: {np.round(z, 4)}')\n",
    "print(f'p value: {np.round(p, 4)}')\n",
    "print(f'95% CI ({np.round(lower, 4)}, {np.round(upper, 5)})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evidence strongly suggests that the proportion of tickets closed through education in the US is lower than the world mean proportion.\n",
    "\n",
    "For our last predictor, tenure, lets create a scatter plot of the mean proportion of successful tickets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-18T23:06:48.005085Z",
     "start_time": "2018-07-18T23:06:44.082Z"
    },
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# prop by tenure\n",
    "x = np.unique(sorted(df.tenure.tolist()))\n",
    "y = []\n",
    "c = []\n",
    "\n",
    "grouped = df.groupby('tenure', axis=0)\n",
    "for key in grouped.groups.keys():\n",
    "    a = np.sum(grouped.get_group(key).edu)\n",
    "    b = len(grouped.get_group(key).edu)\n",
    "    y.append(a/(a+b))\n",
    "    c.append(a+b)\n",
    "\n",
    "scat = go.Scatter(\n",
    "    x = x,\n",
    "    y = y,\n",
    "    mode = 'markers',\n",
    "    marker=dict(\n",
    "        color = c,\n",
    "        colorscale = [[0, Theme().GREY], [1, Theme().ORANGE]],\n",
    "        showscale=True,\n",
    "        opacity=.8\n",
    "    )\n",
    ")\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Proportion of positive response variable increases with tenure',\n",
    "    xaxis=dict(title='Months of tenure'),\n",
    "    yaxis=dict(title='Proportion of tickets closed through education')\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=[scat], layout=layout)\n",
    "iplot(fig, filename='tenure_props')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-18T23:06:48.007424Z",
     "start_time": "2018-07-18T23:06:44.086Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# density\n",
    "mgr = df.loc[df.is_mgr  == True, 'tenure']\n",
    "ic  = df.loc[df.is_ic   == True, 'tenure']\n",
    "hist = [mgr, ic]\n",
    "\n",
    "labels = ['manager', 'IC']\n",
    "colors = [Theme().GREY, Theme().ORANGE]\n",
    "\n",
    "fig = figf.create_distplot(hist, labels, bin_size=[5, 5], colors=colors)\n",
    "fig['layout'].update(title='Density of tenure by career')\n",
    "iplot(fig, filename='tenure_density')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can draw some interesting conclusions from these visualizations. First, it appears as if the proportion of successful tickets begin to increase near 200 months of tenure. But more notably, most of the personnel in the range of increase are managers. Also notice that the majority of tickets being resolved are in the lower end of tenure meaning less mature, and lower ranking personnel are handling most tickets. This is somewhat expected behavior as managers rarely sit as on-call tech support.\n",
    "\n",
    "Another interesting characteristic is that neither IC or manager densities resemble the distribution of what should be a Poisson process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll generate a logistic regression model so we can see how each factor effects the probability of a ticket being closed through education. First, lets create the indicator variables and fit a LogisticRegression with L1 regularization. I'd like to see which variables contribute the most variance and eliminate those that do not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-18T23:06:48.009668Z",
     "start_time": "2018-07-18T23:06:44.091Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# prepare the data\n",
    "x = df.drop(columns=['edu', 'city', 'country', 'position', 'division', 'technician'])\n",
    "y = df.edu.values\n",
    "\n",
    "# generate indicator variables\n",
    "x  = pd.get_dummies(x)\n",
    "        \n",
    "# split and standardize\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42)\n",
    "\n",
    "scaler  = StandardScaler().fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test  = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T09:31:55.125011Z",
     "start_time": "2018-06-12T09:31:55.119148Z"
    }
   },
   "source": [
    "First, lets fit the model without regularization so we can interpret the coefficients. Scikit doesn't provide an option to not regularize so we'll set the regularization parameter to a level such that it'll make no difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-18T23:06:48.012690Z",
     "start_time": "2018-07-18T23:06:44.096Z"
    }
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression(\n",
    "    penalty='l2', \n",
    "    C=1e9, \n",
    "    random_state=42\n",
    ").fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-18T23:06:48.015356Z",
     "start_time": "2018-07-18T23:06:44.100Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# plot feature importance\n",
    "coef = [dict(feature=b, coef=(a)) for a, b in zip(model.coef_[0], x.columns)]\n",
    "coef = sorted(coef, key=lambda x: x['coef'])\n",
    "\n",
    "bar = go.Bar(\n",
    "    x = [x['feature'] for x in coef],\n",
    "    y = [x['coef'] for x in coef],\n",
    "    marker = dict(color = Theme().GREY),\n",
    "    opacity = .9\n",
    ")\n",
    "layout = go.Layout(\n",
    "    title='Feature importance',\n",
    "    xaxis=dict(\n",
    "        title='feature',\n",
    "        tickangle=45\n",
    "    ),\n",
    "    yaxis=dict(title='magnitude of coefficient')\n",
    ")\n",
    "fig = go.Figure(data=[bar], layout=layout)\n",
    "iplot(fig, filename='feature_importance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot makes me want to engineer some more features. Notice the most impacting feature is 'ticket_count', an engineered. This says that, when all other covariates are held constant, the probability of a ticket solved through education diminishes by over 30% for each new ticket! The next most important feature that reduces our probability is an indicator determining whether or not the person worked in an office. On the other end of the spectrum we have a few features that give us a boost: category_apps, building_mobile, and career_ic2 are the top three at .1, .08, and .07 respectively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "453px",
    "left": "777.692px",
    "right": "20px",
    "top": "127px",
    "width": "676px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
